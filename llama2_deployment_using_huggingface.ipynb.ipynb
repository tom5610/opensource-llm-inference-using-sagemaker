{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deploy Llama 2 LLM to Amazon SageMaker using HuggingFace LLM DLC\n",
    "\n",
    "> The notebook is based on [Introducing the Hugging Face LLM Inference Container for Amazon SageMaker](https://huggingface.co/blog/sagemaker-huggingface-llm) from Huggingface blog. However, given llama2 models are private ones, you will need additional steps and setting so as to gain the access and deploy them.\n",
    "\n",
    "> Highly recommended to use SageMaker Studio (with DataScience 2.0 image; t3.medium instance type is good enough.) to run this notebook with proper permissions to use SageMaker services. If you are running on local / others, please consider setup proper AWS credential profile in your environment. \n",
    "\n",
    "## TL;DR;\n",
    "\n",
    "The purpose of the notebook is to provide guidance on how to deploy Llama 2 open source model deployment using SageMaker realtime inference service. Though AWS Machine Learning blog [Llama 2 foundation models from Meta are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/) provides guidance on SageMaker JumpStart deployment, currently llama 2 family modes are only available in Amazon SageMaker Studio in `us-east-1`, `us-west-2`, `eu-west-1` and `ap-southeast-1` regions. If you are planning to deploy the model(s) in other regions, this notebook is a good reference to you. \n",
    "\n",
    "The example covers:\n",
    "1. [Apply Llama 2 models access](#1-apply-llama2-models-access)\n",
    "2. [Setup development environment](#2-setup-development-environment)\n",
    "3. [Retrieve the new Hugging Face LLM DLC](#3-retrieve-the-new-hugging-face-llm-dlc)\n",
    "4. [Deploy llama 2 to Amazon SageMaker](#4-deploy-open-assistant-12b-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with our model](#5-run-inference-and-chat-with-our-model)\n",
    "\n",
    "## What is Hugging Face LLM Inference DLC?\n",
    "\n",
    "Hugging Face LLM DLC is a new purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference), an open-source, purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and dynamic batching for the most popular open-source LLMs, including StarCoder, BLOOM, GPT-NeoX, Llama, and T5. \n",
    "Text Generation Inference is already used by customers such as IBM, Grammarly, and the Open-Assistant initiative implements optimization for all supported model architectures, including:\n",
    "* Tensor Parallelism and custom cuda kernels\n",
    "* Optimized transformers code for inference using [flash-attention](https://github.com/HazyResearch/flash-attention) on the most popular architectures\n",
    "* Quantization with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "* [Continuous batching of incoming requests](https://github.com/huggingface/text-generation-inference/tree/main/router) for increased total throughput\n",
    "* Accelerated weight loading (start-up time) with [safetensors](https://github.com/huggingface/safetensors)\n",
    "* Logits warpers (temperature scaling, topk, repetition penalty ...)\n",
    "* Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\n",
    "* Stop sequences, Log probabilities\n",
    "* Token streaming using Server-Sent Events (SSE)\n",
    "\n",
    "Officially supported model architectures are currently: \n",
    "* [Llama](https://github.com/facebookresearch/llama) (vicuna, alpaca, koala) - ***llama 2 is available as of now***\n",
    "* [BLOOM](https://huggingface.co/bigscience/bloom) / [BLOOMZ](https://huggingface.co/bigscience/bloomz)\n",
    "* [MT0-XXL](https://huggingface.co/bigscience/mt0-xxl)\n",
    "* [Galactica](https://huggingface.co/facebook/galactica-120b)\n",
    "* [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "* [GPT-Neox 20B](https://huggingface.co/EleutherAI/gpt-neox-20b) (joi, pythia, lotus, rosey, chip, RedPajama, open assistant)\n",
    "* [FLAN-T5-XXL](https://huggingface.co/google/flan-t5-xxl) (T5-11B)\n",
    "* [Starcoder](https://huggingface.co/bigcode/starcoder) / [SantaCoder](https://huggingface.co/bigcode/santacoder)\n",
    "* [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) / [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)\n",
    "\n",
    "With the new Hugging Face LLM Inference DLCs on Amazon SageMaker, AWS customers can benefit from the same technologies that power highly concurrent, low latency LLM experiences like [HuggingChat](https://hf.co/chat), [OpenAssistant](https://open-assistant.io/), and Inference API for LLM models on the Hugging Face Hub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apply Llama 2 models access\n",
    "\n",
    "### Llama access application steps\n",
    "\n",
    "> For using HuggingFace Models Hub, please ensure that you are using exactly the same email id on model access application from Meta and HuggingFace account registration.\n",
    "\n",
    "1. To apply access on [Meta Llama Access Form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). Once you submit the request, you may receive the email confirmation on your access.\n",
    "   * Within 1~2 days, Meta may send you an email with model access instructions. There will be a download links being used with [download.sh](https://github.com/facebookresearch/llama/blob/main/download.sh). If the link expires later, you may re-submit your access again so that it will generate a new link for you.\n",
    "   * Once you get the approval, you have two options for access model for your model deployment using SageMaker realtime inference.\n",
    "     * [Preferred] Using SageMaker LLM DLC with HuggingFace Models Hub. (I am referring this track in this notebook)\n",
    "     * Downloading the expected model artifacts, wrapping & uploading to S3 bucket and then using SageMaker realtime inference. (This option may suit for organizations practicing highly-regulated cloud security without internet access; I will follow up with another blog and talk about the security practices on SageMaker realtime inference service.)\n",
    "\n",
    "![meta llama access application](./images/meta_llama_access_application.png)\n",
    "\n",
    "2. To apply access on [HuggingFace Models Hub]. As Meta Llama models are private ones, so you will have to sign up a HuggingFace account and apply access on the models page. e.g. [Llama 2 7B chat-hf model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), the application page for fine-tuned LLMs, called Llama-2-Chat (optimized for dialogue use cases). \n",
    "\n",
    "![huggingface model hug meta llama access application](./images/hugging_face_llama_application.png)\n",
    "\n",
    "  * Once you get the approval from Meta, the HuggingFace one will be approved shortly. \n",
    "\n",
    "3. To generate a READ access token\n",
    "\n",
    "  * Refer to [Access Tokens Setting](https://huggingface.co/settings/tokens); please generate a READ access one so that you can use it for model deployment later.\n",
    "  * Please run below shell script to generate the `.env` file (this is to avoid placing your token as hardcode in the notebook, which you may end up credential leaking with sharing checkin notebook on public repositories.)\n",
    "  * Then copy your token and update the `.env` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"HF_API_TOKEN=\" > ./.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy [Llama 2 7B chat-hf model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) to Amazon SageMaker realtime inference. We need to make sure to have an AWS account configured and install proper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.173.0 (from -r requirements.txt (line 1))\n",
      "  Downloading sagemaker-2.173.0.tar.gz (854 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m854.4/854.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (23.1.0)\n",
      "Collecting boto3<2.0,>=1.26.131 (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for boto3<2.0,>=1.26.131 from https://files.pythonhosted.org/packages/46/a7/487512e3328f2566d72aed3b7059fd8dff18c95d9bcbbe16c5ecc13e6fc5/boto3-1.28.17-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.28.17-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (4.23.4)\n",
      "Collecting smdebug_rulesconfig==1.0.1 (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: pathos in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (0.3.1)\n",
      "Collecting schema (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from sagemaker==2.173.0->-r requirements.txt (line 1)) (3.9.1)\n",
      "Collecting tblib==1.7.0 (from sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting botocore<1.32.0,>=1.31.17 (from boto3<2.0,>=1.26.131->sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for botocore<1.32.0,>=1.31.17 from https://files.pythonhosted.org/packages/3d/e5/32a88f5a95e3d43c2e3ed86fc1ffdb715547a04f95a51d00e1185af63b0c/botocore-1.31.17-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.173.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.173.0->-r requirements.txt (line 1)) (3.16.2)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from google-pasta->sagemaker==2.173.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from jsonschema->sagemaker==2.173.0->-r requirements.txt (line 1)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from jsonschema->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from jsonschema->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pandas->sagemaker==2.173.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pandas->sagemaker==2.173.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pandas->sagemaker==2.173.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pathos->sagemaker==2.173.0->-r requirements.txt (line 1)) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pathos->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pathos->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from pathos->sagemaker==2.173.0->-r requirements.txt (line 1)) (0.70.15)\n",
      "Collecting contextlib2>=0.5.5 (from schema->sagemaker==2.173.0->-r requirements.txt (line 1))\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages (from botocore<1.32.0,>=1.31.17->boto3<2.0,>=1.26.131->sagemaker==2.173.0->-r requirements.txt (line 1)) (1.26.15)\n",
      "Downloading boto3-1.28.17-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.173.0-py2.py3-none-any.whl size=1163282 sha256=6bf9994fce59eba6b86ca2510ebb907f237503aa8fb1ff6aafd0388b33780b45\n",
      "  Stored in directory: /Users/tomlu/Library/Caches/pip/wheels/8c/49/ed/039aaacbad678bf5fa498e3c972a2ee51d87a6c1bd0c715a5c\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: tblib, smdebug_rulesconfig, google-pasta, contextlib2, cloudpickle, schema, botocore, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.76\n",
      "    Uninstalling botocore-1.29.76:\n",
      "      Successfully uninstalled botocore-1.29.76\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.76\n",
      "    Uninstalling boto3-1.26.76:\n",
      "      Successfully uninstalled boto3-1.26.76\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.31.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.28.17 botocore-1.31.17 cloudpickle-2.2.1 contextlib2-21.6.0 google-pasta-0.2.0 sagemaker-2.173.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tblib-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve the new Hugging Face LLM DLC\n",
    "\n",
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
